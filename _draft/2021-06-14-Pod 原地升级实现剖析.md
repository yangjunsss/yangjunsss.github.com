---
description: "
总结归纳了下 Pod 原地升级实现原理和现状
"
---

### 背景
在 K8 的设计中，Pod 升级应该推荐能任意重建，这对无状态服务比较友善，但对有状态 stateful 和 batch 批量任务等场景中，用户更希望 Pod 原地升级，保持 PV 和 IP 不变，比如如下场景：
1. 资源受限下无法扩容滚动升级
2. Pod 保持 IP 和 PV 不变，能快速升级 Pod
3. SideCar Container 升级无需重启宿主 Container
4. VPA 扩容 Limit&Request 而无需重启 Pod

in-place 原地升级的诉求早在 2015 年社区就有人提出 [issue#9043](https://github.com/kubernetes/kubernetes/issues/9043)，讨论激烈，主要诉求：
1. Container Image in-place 更新无需重启 Pod
2. Container Limit&Request 更新无需重启 Pod

其中 #1 腾讯已经[实现](https://cloud.tencent.com/developer/article/1413743)， [阿里 Kruise 实现](https://www.alibabacloud.com/blog/explaining-in-place-pod-updates-in-a-kubernetes-cluster_597133)

其中 #2 在 2016 年 docker 1.10 支持 [live update resource constraints](https://docker.com/blog/docker-1-10/) ，社区 2018 年开始着手设计 [Pod VPA](https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1287-in-place-update-pod-resources)，目前有望合入 v1.22 版本。

### Container Image In-place 实现的理论基础
之所以能够使 Pod 原地升级 Container 的基础其实是 kubelet 的 syncPod 主流程中会对 Node 上 *spec.container[x].image* 与 kube-api 中的 *spec.container[x].image* 做一致性 hash 检查，一旦发现不一致就触发 container 重建，这也是 K8 用来保证环境一致性的关键措施。同时通过 *status.containerStatues[x].image* 来观察实际 container 的 image 状态，最终就完成了原地升级。v1.16 后 hash 的计算如下（与 v1.15 之前有差别，升级后会导致 Container 重启）：

pkg/kubelet/container/helpers.go
```go
func HashContainer(container *v1.Container) uint64 {
	hash := fnv.New32a()
	// Omit nil or empty field when calculating hash value
	// Please see https://github.com/kubernetes/kubernetes/issues/53644
	containerJson,_ := json.Marshal(container)
	hashutil.DeepHashObject(hash, containerJson)
	return uint64(hash.Sum32())
}
```
kubelet 启动后的一致性检查逻辑如下：
pkg/kubelet/kuberruntime/kuberruntime_manager.go
```go
func (m *kubeGenericRuntimeManager) computePodActions(pod *v1.Pod, podStatus *kubecontainer.PodStatus) podActions {
	// calculating and equalling by hash 
	if _,_,changed := containerChanged(&container, containerStatus);changed {
        restart = true
    }
    ...
	if restart {
		// put the container idx into starting queue
		changes.ContainersToStart = append(changes.ContainersToStart, idx)
    }
    ...
	// put the container into killing queue
	changes.ContainersToKill[containerStatus.ID] = containerToKillInfo {
	    name: containerStatus.Name,
	    container: &pod.Spec.Containers[idx],
    }   
}
```
后续就是 kill 和 start 的流程了。

### Kruise 的增强实现
kruise 在 *manager* 的 controller 中通过 patch 更新 Pod 的 *spec.containers[x].image* 字段来触发 kubelet 的重建 container 动作：
pkg/util/inplaceupdate/options.go
```go
func defaultPatchUpdateSpecToPod(pod *v1.Pod, spec *UpdateSpec) (*v1.Pod,error) {
	...
	for i := range pod.Spec.Containers {
	    if newImage,ok := spec.ContainerImages[pod.Spec.Container[i].Name];ok {
	    	pod.Spec.Containers[i].Image = newImage
        }   
    }   
    return pod, nil
}
```
1. 通过 patch 更新 kube-api Pod Spec
2. 通过 *status.containerStatuses[x].image* 判断更新状态
3. 通过 *spec.readinessGates* 来控制导流

详尽的技术分析见 Kruise 作者之一 Siyu Wang 的一篇[技术博客](https://www.alibabacloud.com/blog/explaining-in-place-pod-updates-in-a-kubernetes-cluster_597133)

### Controller 与 kubelet 的配合
如果没有自己的 Controller，强行更改 Pod Spec，会导致上层 SS 在更新的时候又重置回去，而如果直接更改 SS 的 Spec，实际导致的又是 Pod 重建，因此必须两者一起配合，Kruise 是自己实现的 Controller。

### Pod Limit&Request 原地更新
Limit&Request 更新牵涉到 Scheduler 的调度调整和 CRI 的接口变化，虽然底层 Container 已经支持了 cgroup 的动态修改，多方的配合导致实现这个要复杂一点。社区 2018 年开始起草设计 [Pod VPA](https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1287-in-place-update-pod-resources)，修改的范围涉及到 kubelet、kube-scheduler、kube-controller，新增 Pod Spec API 如下：
1. *spec.containers[x].resources.request[cpu] = x // old filed，表明当前期望需要的 request
2. *status.containerStatuses[x].resourcesAllocated[cpu] = x // new filed，表明需要申请的 request
3. *status.resize[cpu] = x // new filed，当前的状态，包括 Proposed 申请中、InProgress 更新中、Infeasible 没资源拒绝、Deferred 有资源延迟
4. *status.containerStatuses[x].resources.request[cpu] = x // new filed,实际的 request

1. kubelet 要实现 Resize 相关的 CRI，调用底层 Resize 的能力
2. Controller 要实现对应的 Admission 等权限、配额等管控能力
3. Scheduler 要配合新的 *resourcesAllocated* 字段重新计算并调度

### 总结和展望
在某些特性上，社区的进度还是太慢了，深入细节才能激发创新力。